Question:

Reply in Chinese (Simplified).
The following is the text content of a web page, analyze the core content and summarize:
OpenTAD: An Open-Source Temporal Action Detection Toolbox.
OpenTADï¼šå¼€æºæ—¶é—´åŠ¨ä½œæ£€æµ‹å·¥å…·ç®±ã€‚OpenTAD is an open-source temporal action detection (TAD) toolbox based on PyTorch.
OpenTAD æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„å¼€æºæ—¶é—´åŠ¨ä½œæ£€æµ‹ (TAD) å·¥å…·ç®±ã€‚ğŸ¥³ What's NewÂ ğŸ¥³ æœ€æ–°æ¶ˆæ¯
A technical report of this library will be provided soon.
è¯¥åº“çš„æŠ€æœ¯æŠ¥å‘Šå°†å¾ˆå¿«æä¾›ã€‚
2024/03/28: The beta version v0.1.0 of OpenTAD is released. Any feedbacks and suggestions are welcome!
2024/03/28ï¼šOpenTAD betaç‰ˆæœ¬v0.1.0å‘å¸ƒã€‚æ¬¢è¿ä»»ä½•åé¦ˆå’Œå»ºè®®ï¼
ğŸ“– Major FeaturesÂ ğŸ“– ä¸»è¦ç‰¹ç‚¹
Support SoTA TAD methods with modular design. We decompose the TAD pipeline into different components, and implement them in a modular way. This design makes it easy to implement new methods and reproduce existing methods.
æ”¯æŒSoTA TADæ–¹æ³•ï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ã€‚æˆ‘ä»¬å°†TADç®¡é“åˆ†è§£ä¸ºä¸åŒçš„ç»„ä»¶ï¼Œå¹¶ä»¥æ¨¡å—åŒ–çš„æ–¹å¼å®ç°å®ƒä»¬ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å®ç°æ–°æ–¹æ³•å’Œé‡ç°ç°æœ‰æ–¹æ³•å˜å¾—å®¹æ˜“ã€‚
Support multiple TAD datasets. We support 8 TAD datasets, including ActivityNet-1.3, THUMOS-14, HACS, Ego4D-MQ, Epic-Kitchens-100, FineAction, Multi-THUMOS, Charades datasets.
æ”¯æŒå¤šä¸ªTADæ•°æ®é›†ã€‚æˆ‘ä»¬æ”¯æŒ 8 ä¸ª TAD æ•°æ®é›†ï¼ŒåŒ…æ‹¬ ActivityNet-1.3ã€THUMOS-14ã€HACSã€Ego4D-MQã€Epic-Kitchens-100ã€FineActionã€Multi-THUMOSã€Charades æ•°æ®é›†ã€‚
Support feature-based training and end-to-end training. The feature-based training can easily be extended to end-to-end training with raw video input, and the video backbone can be easily replaced.
æ”¯æŒåŸºäºç‰¹å¾çš„è®­ç»ƒå’Œç«¯åˆ°ç«¯è®­ç»ƒã€‚åŸºäºç‰¹å¾çš„è®­ç»ƒå¯ä»¥è½»æ¾æ‰©å±•åˆ°ä½¿ç”¨åŸå§‹è§†é¢‘è¾“å…¥çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¹¶ä¸”è§†é¢‘ä¸»å¹²å¯ä»¥è½»æ¾æ›¿æ¢ã€‚
Release various pre-extracted features. We release the feature extraction code, as well as many pre-extracted features on each dataset.
é‡Šæ”¾å„ç§é¢„å…ˆæå–çš„åŠŸèƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†ç‰¹å¾æå–ä»£ç ï¼Œä»¥åŠæ¯ä¸ªæ•°æ®é›†ä¸Šçš„è®¸å¤šé¢„å…ˆæå–çš„ç‰¹å¾ã€‚
ğŸŒŸ Model ZooÂ ğŸŒŸ æ¨¡å‹åŠ¨ç‰©å›­
One StageTwo StageDETREnd-to-End TrainingÂ ç«¯åˆ°ç«¯åŸ¹è®­ActionFormer (ECCV'22)Â è¡ŒåŠ¨è€… (ECCV'22)
TriDet (CVPR'23)Â  ä¸‰å¾· (CVPR'23)
TemporalMaxer (arXiv'23)
VideoMambaSuite (arXiv'24)
è§†é¢‘MambaSuite (arXiv'24)BMN (ICCV'19)
GTAD (CVPR'20)
TSI (ACCV'20)
VSGN (ICCV'21)TadTR (TIP'22)AFSD (CVPR'21)
E2E-TAD (CVPR'22)
ETAD (CVPRW'23)
Re2TAL (CVPR'23)
AdaTAD (CVPR'24)Â  é˜¿è¾¾å¡”å¾· (CVPR'24)The detailed configs, results, and pretrained models of each method can be found in above folders.
æ¯ç§æ–¹æ³•çš„è¯¦ç»†é…ç½®ã€ç»“æœå’Œé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥åœ¨ä¸Šé¢çš„æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ã€‚ğŸ› ï¸ InstallationÂ ğŸ› ï¸å®‰è£…Please refer to install.md for installation and data preparation.
å®‰è£…å’Œæ•°æ®å‡†å¤‡è¯·å‚è€ƒinstall.mdã€‚ğŸš€ UsageÂ ğŸš€ ç”¨æ³•Please refer to usage.md for details of training and evaluation scripts.
è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬çš„è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒusage.mdã€‚ğŸ“„ UpdatesÂ ğŸ“„ æ›´æ–°Please refer to changelog.md for update details.
æ›´æ–°è¯¦æƒ…è¯·å‚è€ƒchangelog.mdã€‚ğŸ¤ RoadmapÂ ğŸ¤ è·¯çº¿å›¾All the things that need to be done in the future is in roadmap.md.
ä»¥åæ‰€æœ‰éœ€è¦åšçš„äº‹æƒ…éƒ½åœ¨roadmap.mdä¸­ã€‚ğŸ–Šï¸ CitationÂ ğŸ–Šï¸å¼•ç”¨[Acknowledgement] This repo is inspired by OpenMMLab project, and we give our thanks to their contributors.
[è‡´è°¢] è¿™ä¸ªå­˜å‚¨åº“å—åˆ° OpenMMLab é¡¹ç›®çš„å¯å‘ï¼Œæˆ‘ä»¬æ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®è€…ã€‚If you think this repo is helpful, please cite us:
å¦‚æœæ‚¨è®¤ä¸ºæ­¤å­˜å‚¨åº“æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬ï¼š@misc{2024opentad,
title={OpenTAD: An Open-Source Toolbox for Temporal Action Detection},
author={Shuming Liu, Chen Zhao, Fatimah Zohra, Mattia Soldan, Carlos Hinojosa, Alejandro Pardo, Anthony Cioppa, Lama Alssum, Mengmeng Xu, Merey Ramazanova, Juan LeÃ³n AlcÃ¡zar, Silvio Giancola, Bernard Ghanem},
howpublished = {\url{https://github.com/sming256/opentad}},
year={2024}
}If you have any questions, please contact: shuming.liu@kaust.edu.sa.
å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·è”ç³»ï¼š shuming.liu@kaust.edu.sa ã€‚

Answer:

**OpenTAD** æ˜¯ä¸€ä¸ªåŸºäº **PyTorch** çš„å¼€æºæ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰å·¥å…·ç®±ã€‚å®ƒæ”¯æŒæœ€å…ˆè¿›çš„ TAD æ–¹æ³•ï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå°† TAD ç®¡é“åˆ†è§£ä¸ºä¸åŒçš„ç»„ä»¶ï¼Œå¹¶ä»¥æ¨¡å—åŒ–çš„æ–¹å¼å®ç°å®ƒä»¬ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å®ç°æ–°æ–¹æ³•å’Œé‡ç°ç°æœ‰æ–¹æ³•å˜å¾—å®¹æ˜“ã€‚æ­¤å¤–ï¼ŒOpenTAD è¿˜æ”¯æŒå¤šä¸ª TAD æ•°æ®é›†ï¼ŒåŒ…æ‹¬ **ActivityNet-1.3ã€THUMOS-14ã€HACSã€Ego4D-MQã€Epic-Kitchens-100ã€FineActionã€Multi-THUMOS å’Œ Charades** æ•°æ®é›†<sup>1</sup>ã€‚

å¦‚æœæ‚¨å¯¹æ—¶é—´åŠ¨ä½œæ£€æµ‹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨ [OpenTAD GitHub é¡µé¢](https://github.com/sming256/openTAD) æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯å’Œä½¿ç”¨è¯´æ˜ã€‚ğŸš€
\-
\[1]: [OpenTAD: An Open-Source Temporal Action Detection Toolbox.](https://github.com/sming256/openTAD)
\[2]: [The official implementation of AdaTAD: End-to-End Temporal Action ...](https://github.com/sming256/AdaTAD)
\[3]: [temporal-action-detection Â· GitHub Topics Â· GitHub](https://github.com/topics/temporal-action-detection)
\[4]: [OpenTAD/README.md at main Â· sming256/OpenTAD Â· GitHub](https://github.com/sming256/OpenTAD/blob/main/README.md)
\[5]: [Actions Â· sming256/OpenTAD Â· GitHub](https://github.com/sming256/OpenTAD/actions)

<hr/>

Question:

æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

Answer:

**æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰** æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘ç†è§£ä»»åŠ¡ã€‚å®ƒçš„ç›®æ ‡æ˜¯åœ¨æœªå‰ªè¾‘çš„è§†é¢‘ä¸­åŒæ—¶é¢„æµ‹æ¯ä¸ªåŠ¨ä½œå®ä¾‹çš„è¯­ä¹‰æ ‡ç­¾å’Œæ—¶é—´é—´éš”ã€‚å…·ä½“æ¥è¯´ï¼ŒTAD éœ€è¦ç¡®å®šç‰¹å®šåŠ¨ä½œå®ä¾‹çš„å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚è¿™ä¸ªä»»åŠ¡ç›¸å¯¹äºåŠ¨ä½œè¯†åˆ«æ¥è¯´æ›´åŠ å¤æ‚ï¼Œå› ä¸ºæ¨¡å‹ä¸ä»…éœ€è¦è¾“å‡ºåŠ¨ä½œç±»åˆ«ï¼Œè¿˜éœ€è¦ç²¾ç¡®åœ°ç¡®å®šå¼€å§‹å’Œç»“æŸçš„æ—¶é—´ç‚¹<sup>1</sup> <sup>3</sup>ã€‚

å¦‚æœæ‚¨å¯¹æ—¶é—´åŠ¨ä½œæ£€æµ‹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨ [è¿™é‡Œ](https://arxiv.org/abs/2106.10271) é˜…è¯»ä¸€ç¯‡å…³äºåŸºäº Transformer çš„ TAD æ–¹æ³•çš„è®ºæ–‡ï¼Œè¯¥æ–¹æ³•åœ¨ THUMOS14 å’Œ HACS Segments æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬è¾ƒä¹‹å‰çš„æ£€æµ‹å™¨æ›´ä½<sup>1</sup>ã€‚
\-
\[1]: [End-to-end Temporal Action Detection with Transformer](https://arxiv.org/abs/2106.10271)
\[2]: [An Empirical Study of End-to-End Temporal Action Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_An_Empirical_Study_of_End-to-End_Temporal_Action_Detection_CVPR_2022_paper.pdf)
\[3]: [TadML: A Fast Temporal Action Detection with Mechanics-MLP](https://link.springer.com/chapter/10.1007/978-3-031-50069-5_4)
\[4]: [Overview of temporal action detection based on deep learning - Springer](https://link.springer.com/article/10.1007/s10462-023-10650-w)
\[5]: [MCMNET: Multi-Scale Context Modeling Network for Temporal Action Detection](https://www.mdpi.com/1424-8220/23/17/7563)
\[6]: [Temporal Action Detection with Structured Segment Networks](https://link.springer.com/article/10.1007/s11263-019-01211-2)
\[7]: [Actions as points: a simple and efficient detector for ... - Springer](https://link.springer.com/article/10.1007/s00138-023-01377-3)
\[8]: [Temporal Action Localization | Papers With Code](https://paperswithcode.com/task/action-recognition)

<hr/>

Question:

æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰èƒ½åšä»€ä¹ˆï¼Ÿè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

Answer:

**æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰** åœ¨è§†é¢‘ç†è§£é¢†åŸŸä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº› TAD çš„åº”ç”¨åœºæ™¯ï¼š

1. **è§†é¢‘ç›‘æ§**ï¼šTAD å¯ä»¥å¸®åŠ©è‡ªåŠ¨å®šä½æœªå‰ªè¾‘è§†é¢‘ä¸­çš„åŠ¨ä½œå¼€å§‹å’Œç»“æŸæ—¶é—´ï¼Œä»è€Œåœ¨è§†é¢‘ç›‘æ§ç³»ç»Ÿä¸­è¯†åˆ«ç‰¹å®šçš„è¡Œä¸ºæˆ–äº‹ä»¶ã€‚

2. **å†…å®¹åˆ†æå’Œæ‘˜è¦**ï¼šTAD å¯ä»¥ç”¨äºè‡ªåŠ¨æå–è§†é¢‘ä¸­çš„å…³é”®åŠ¨ä½œæˆ–äº‹ä»¶ï¼Œä»è€Œç”Ÿæˆè§†é¢‘æ‘˜è¦æˆ–é«˜äº®ç‰‡æ®µã€‚

3. **äººæœºäº¤äº’**ï¼šTAD å¯ä»¥å¸®åŠ©è®¡ç®—æœºç†è§£äººç±»åœ¨è§†é¢‘ä¸­çš„è¡Œä¸ºï¼Œä»è€Œæ”¹å–„äººæœºäº¤äº’ä½“éªŒã€‚

4. **æ™ºèƒ½å®‰å…¨**ï¼šTAD åœ¨æ™ºèƒ½å®‰å…¨é¢†åŸŸä¸­ä¹Ÿæœ‰åº”ç”¨ï¼Œä¾‹å¦‚æ£€æµ‹å¼‚å¸¸è¡Œä¸ºã€å…¥ä¾µæ£€æµ‹ç­‰ã€‚

æ€»ä¹‹ï¼ŒTAD åœ¨å¤šä¸ªé¢†åŸŸä¸­éƒ½å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£å’Œåˆ†æè§†é¢‘æ•°æ®ã€‚
\-
\[1]: [Electronics | Free Full-Text | Advancing Temporal Action Localization ...](https://bing.com/search?q=temporal+action+detection+applications)
\[2]: [TadML: A Fast Temporal Action Detection with Mechanics-MLP](https://link.springer.com/chapter/10.1007/978-3-031-50069-5_4)
\[3]: [Improving accuracy of temporal action detection by deep hybrid ...](https://link.springer.com/article/10.1007/s11042-022-13962-1)
\[4]: [Temporal action detection | Visionary AI Research (VAR) Group](https://jiminliang.github.io/highlights-cv/001-action/)
\[5]: [Overview of temporal action detection based on deep learning - Springer](https://link.springer.com/article/10.1007/s10462-023-10650-w)

<hr/>

Question:

TAD æ•°æ®é›†

Answer:

å…³äº**æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰** çš„æ•°æ®é›†æœ‰å¾ˆå¤šï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„ TAD æ•°æ®é›†ï¼š

1. **ActivityNet**ï¼šè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„åŠ¨ä½œç±»åˆ«å’Œæœªå‰ªè¾‘è§†é¢‘ç‰‡æ®µã€‚å®ƒè¢«å¹¿æ³›ç”¨äºåŠ¨ä½œè¯†åˆ«å’Œæ—¶é—´åŠ¨ä½œæ£€æµ‹ä»»åŠ¡<sup>1</sup>ã€‚

2. **Charades**ï¼šCharades æ•°æ®é›†åŒ…å«äººç±»åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ‰§è¡Œçš„å„ç§åŠ¨ä½œï¼Œä¾‹å¦‚æ‰“ç”µè¯ã€åšé¥­ã€æ‰“æ‰«ç­‰ã€‚å®ƒæ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾çš„æ•°æ®é›†ï¼Œé€‚ç”¨äºæ—¶é—´åŠ¨ä½œæ£€æµ‹å’Œå…¶ä»–è§†é¢‘ç†è§£ä»»åŠ¡<sup>2</sup>ã€‚

3. **THUMOS14**ï¼šè¿™æ˜¯ä¸€ä¸ªç”¨äºæ—¶é—´åŠ¨ä½œæ£€æµ‹çš„ç»å…¸æ•°æ®é›†ï¼ŒåŒ…å«äº†å„ç§ä¸åŒç±»å‹çš„åŠ¨ä½œå®ä¾‹ã€‚å®ƒè¢«å¹¿æ³›ç”¨äºè¯„ä¼° TAD æ–¹æ³•çš„æ€§èƒ½<sup>1</sup>ã€‚

4. **FineAction**ï¼šFineAction æ˜¯ä¸€ä¸ªç»†ç²’åº¦è§†é¢‘æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºæ—¶é—´åŠ¨ä½œå®šä½ä»»åŠ¡ã€‚å®ƒåŒ…å«äº†ä¸°å¯Œçš„ç»†ç²’åº¦åŠ¨ä½œå®ä¾‹ï¼Œé€‚ç”¨äºæŒ‘æˆ˜æ€§çš„æ—¶é—´åŠ¨ä½œæ£€æµ‹ç ”ç©¶<sup>3</sup>ã€‚

è¿™äº›æ•°æ®é›†å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘å’Œè¯„ä¼°æ—¶é—´åŠ¨ä½œæ£€æµ‹ç®—æ³•ï¼Œå¹¶æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚
\-
\[1]: [Temporal Action Localization | Papers With Code](https://paperswithcode.com/task/action-recognition)
\[2]: [Action Detection | Papers With Code](https://paperswithcode.com/task/action-detection)
\[3]: [FineAction: A Fine-Grained Video Dataset for Temporal Action Localization](https://arxiv.org/pdf/2105.11107.pdf)
\[4]: [Overview of temporal action detection based on deep learning - Springer](https://link.springer.com/article/10.1007/s10462-023-10650-w)

<hr/>

Question:

ActivityNet-1.3 æ•°æ®é›†

Answer:

**ActivityNet-1.3** æ˜¯ä¸€ä¸ªç”¨äºæ—¶é—´åŠ¨ä½œå®šä½ï¼ˆTemporal Action Localizationï¼‰çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ—¶é—´åŠ¨ä½œæ£€æµ‹ç®—æ³•çš„æ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯å…³äº ActivityNet-1.3 æ•°æ®é›†çš„ä¸€äº›ä¿¡æ¯ï¼š

- **æ•°æ®é›†è§„æ¨¡**ï¼šActivityNet-1.3 åŒ…å«äº†è¶…è¿‡ **19994** ä¸ªæœªå‰ªè¾‘è§†é¢‘ï¼Œæ€»è®¡çº¦ **648** å°æ—¶çš„è§†é¢‘å†…å®¹ã€‚è¿™äº›è§†é¢‘æ¶µç›–äº†å¤§çº¦ **200** ç§ä¸åŒçš„æ—¥å¸¸æ´»åŠ¨ï¼Œä¾‹å¦‚â€œé›ç‹—â€ã€â€œè·³è¿œâ€å’Œâ€œæ¸…æ‰«åœ°æ¿â€ç­‰<sup>2</sup> <sup>4</sup>ã€‚

- **æ•°æ®é›†åˆ’åˆ†**ï¼šActivityNet-1.3 æ•°æ®é›†è¢«åˆ’åˆ†ä¸ºä¸‰ä¸ªä¸ç›¸äº¤çš„å­é›†ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸º **2:1:1**ã€‚æ¯ä¸ªæ´»åŠ¨ç±»åˆ«å¹³å‡æœ‰ **137** ä¸ªæœªå‰ªè¾‘è§†é¢‘ã€‚æ¯ä¸ªè§†é¢‘å¹³å‡åŒ…å« **1.41** ä¸ªå¸¦æœ‰æ—¶é—´è¾¹ç•Œæ³¨é‡Šçš„æ´»åŠ¨<sup>4</sup>ã€‚

å¦‚æœæ‚¨å¯¹ ActivityNet-1.3 æ•°æ®é›†æ„Ÿå…´è¶£ï¼Œæ‚¨å¯ä»¥ä»ä»¥ä¸‹é“¾æ¥ä¸­ä¸‹è½½æ•°æ®ï¼š

- [ActivityNet æ•°æ®ä¸‹è½½](http://activity-net.org/download.html)
- [GitHub ä¸Šçš„ ActivityNet æ•°æ®ä¸‹è½½](https://github.com/UCASUSTC/ActivityNet_Dataset_Download)

æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨å¼€æºå·¥å…· FiftyOne æ¥åŠ è½½ã€å¯è§†åŒ–å’Œè¯„ä¼° ActivityNet æ•°æ®é›†<sup>5</sup>ã€‚
\-
\[1]: [ActivityNet Temporal Action Localization](http://activity-net.org/challenges/2021/tasks/anet_localization.html)
\[2]: [ActivityNet Dataset | Papers With Code](https://paperswithcode.com/dataset/activitynet)
\[3]: [Download - Activity Net](http://activity-net.org/download.html)
\[4]: [PaddleVideo/docs/zh-CN/dataset/ActivityNet.md at develop - GitHub](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/dataset/ActivityNet.md)
\[5]: [UCASUSTC/ActivityNet_Dataset_Download - GitHub](https://github.com/UCASUSTC/ActivityNet_Dataset_Download)
\[6]: [Activity Net](http://activity-net.org/)

<hr/>

Question:

æœªå‰ªè¾‘è§†é¢‘ç‰‡æ®µ

Answer:

**æœªå‰ªè¾‘è§†é¢‘ç‰‡æ®µ**ï¼ˆUntrimmed Video Clipsï¼‰æ˜¯æŒ‡åœ¨æ—¶é—´ä¸Šæ²¡æœ‰å‰ªè¾‘æˆ–è£å‰ªçš„è§†é¢‘ç‰‡æ®µã€‚è¿™äº›è§†é¢‘ç‰‡æ®µé€šå¸¸åŒ…å«å®Œæ•´çš„æ´»åŠ¨æˆ–äº‹ä»¶ï¼Œè€Œä¸ä»…ä»…æ˜¯æŸä¸ªç‰¹å®šçš„åŠ¨ä½œæˆ–è¡Œä¸ºã€‚

åœ¨æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰ä»»åŠ¡ä¸­ï¼Œç ”ç©¶äººå‘˜é€šå¸¸ä½¿ç”¨æœªå‰ªè¾‘è§†é¢‘ç‰‡æ®µæ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚è¿™äº›è§†é¢‘ç‰‡æ®µå¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ å¦‚ä½•åœ¨é•¿æ—¶é—´çš„è§†é¢‘ä¸­å‡†ç¡®åœ°å®šä½å’Œè¯†åˆ«ç‰¹å®šçš„åŠ¨ä½œæˆ–æ´»åŠ¨ã€‚

å¦‚æœæ‚¨å¯¹æœªå‰ªè¾‘è§†é¢‘ç‰‡æ®µæ„Ÿå…´è¶£ï¼Œæ‚¨å¯ä»¥é˜…è¯»ä¸€äº›ç›¸å…³çš„ç ”ç©¶è®ºæ–‡ï¼Œä¾‹å¦‚ï¼š

- [Multi-model learning by sequential reading of untrimmed videos for action recognition](https://arxiv.org/abs/2401.14675)
- [Learning from Weakly-Labeled Videos via Sub-Concepts](https://blog.research.google/2022/03/learning-from-weakly-labeled-videos-via.html)
- [End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos](https://cs.stanford.edu/groups/vision/pdf/buch2017bmvc.pdf)

è¿™äº›è®ºæ–‡ä»‹ç»äº†å¦‚ä½•å¤„ç†æœªå‰ªè¾‘è§†é¢‘ç‰‡æ®µä»¥è¿›è¡Œæ—¶é—´åŠ¨ä½œæ£€æµ‹å’Œå…¶ä»–ç›¸å…³ä»»åŠ¡ã€‚
\-
\[1]: [Multi-model learning by sequential reading of untrimmed videos for ...](https://arxiv.org/abs/2401.14675)
\[2]: [Learning from Weakly-Labeled Videos via Sub-Concepts](https://blog.research.google/2022/03/learning-from-weakly-labeled-videos-via.html)
\[3]: [End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos](https://cs.stanford.edu/groups/vision/pdf/buch2017bmvc.pdf)

<hr/>

Question:

æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰æ¨¡å‹

Answer:

**æ—¶é—´åŠ¨ä½œæ£€æµ‹ï¼ˆTADï¼‰æ¨¡å‹**ç”¨äºåœ¨æœªå‰ªè¾‘çš„è§†é¢‘ä¸­å‡†ç¡®åœ°å®šä½å’Œè¯†åˆ«ç‰¹å®šçš„åŠ¨ä½œæˆ–æ´»åŠ¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„ TAD æ¨¡å‹å’Œæ–¹æ³•ï¼š

1. **Temporal Pooling**ï¼šè¿™æ˜¯ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ—¶é—´ç»´åº¦ä¸Šçš„ç‰¹å¾è¿›è¡Œæ± åŒ–æ¥æ•æ‰åŠ¨ä½œçš„æ—¶åºä¿¡æ¯ã€‚

2. **Temporal Convolution**ï¼šç±»ä¼¼äºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒåœ¨æ—¶é—´ç»´åº¦ä¸Šåº”ç”¨å·ç§¯æ“ä½œï¼Œä»¥æ•è·å±€éƒ¨æ—¶åºæ¨¡å¼ã€‚

3. **Recurrent Neural Networks (RNNs)**ï¼šRNNs æ˜¯ä¸€ç±»é€’å½’ç¥ç»ç½‘ç»œï¼Œå¯ä»¥å¤„ç†å˜é•¿åºåˆ—æ•°æ®ã€‚åœ¨ TAD ä¸­ï¼Œå®ƒä»¬å¯ä»¥ç”¨äºå»ºæ¨¡è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯ã€‚

4. **Temporal Attention Models**ï¼šè¿™äº›æ¨¡å‹ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨è§†é¢‘ä¸­ä¸åŠ¨ä½œç›¸å…³çš„é‡è¦æ—¶é—´æ®µã€‚

5. **Structured Segment Networks**ï¼šè¿™æ˜¯ä¸€ç§ç»“æ„åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶è¾“å‡ºåŠ¨ä½œç±»åˆ«å’Œå…¶ç²¾ç¡®çš„å¼€å§‹å’Œç»“æŸæ—¶é—´ç‚¹ã€‚

å¦‚æœæ‚¨å¯¹ TAD æ¨¡å‹æ„Ÿå…´è¶£ï¼Œæ‚¨å¯ä»¥é˜…è¯»ä¸€äº›ç›¸å…³çš„ç ”ç©¶è®ºæ–‡ï¼Œä¾‹å¦‚ï¼š

- [An empirical study on temporal modeling for online action detection](https://link.springer.com/article/10.1007/s40747-021-00534-3)
- [TadML: A Fast Temporal Action Detection with Mechanics-MLP](https://link.springer.com/chapter/10.1007/978-3-031-50069-5_4)
- [Temporal Action Detection with Structured Segment Networks](https://link.springer.com/article/10.1007/s11263-019-01211-2)
- [Overview of temporal action detection based on deep learning](https://link.springer.com/article/10.1007/s10462-023-10650-w)
- [Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions](https://arxiv.org/abs/2403.20254v1)

æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥åœ¨ [Papers With Code ä¸ŠæŸ¥çœ‹ç›¸å…³ä»»åŠ¡å’Œæ¨¡å‹](https://paperswithcode.com/task/action-recognition)ã€‚
\-
\[1]: [An empirical study on temporal modeling for online action detection ...](https://link.springer.com/article/10.1007/s40747-021-00534-3)
\[2]: [TadML: A Fast Temporal Action Detection with Mechanics-MLP](https://link.springer.com/chapter/10.1007/978-3-031-50069-5_4)
\[3]: [Temporal Action Detection with Structured Segment Networks](https://link.springer.com/article/10.1007/s11263-019-01211-2)
\[4]: [Overview of temporal action detection based on deep learning - Springer](https://link.springer.com/article/10.1007/s10462-023-10650-w)
\[5]: [Benchmarking the Robustness of Temporal Action Detection Models Against ...](https://arxiv.org/abs/2403.20254v1)
\[6]: [Temporal Action Localization | Papers With Code](https://paperswithcode.com/task/action-recognition)

<hr/>

