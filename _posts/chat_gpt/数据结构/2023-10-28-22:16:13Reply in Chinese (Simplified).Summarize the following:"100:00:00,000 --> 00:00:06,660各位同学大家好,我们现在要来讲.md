Question:

Reply in Chinese (Simplified).Summarize the following:"100:00:00,000 --> 00:00:06,660各位同学大家好,我们现在要来讲这个Transformer。200:00:06,660 --> 00:00:14,080那Transformer是什么呢?Transformer它的英文的意思就是变形金刚。300:00:14,080 --> 00:00:34,720那这个变形金刚呢,这个Transformer现在有一个非常知名的应用,这个非常知名的应用叫做BERT,我们把它画成像是一个巨大,超巨大巨人一样,从城墙后面看出来,这个东西就是BERT。400:00:34,720 --> 00:00:40,140好,那我们今天还不会讲到BERT,我们就先讲TRANSFORMER就好了。500:00:40,140 --> 00:00:45,600那BERT是什么呢?BERT就是Uunsupervised train的transformer600:00:45,600 --> 00:00:52,040那transformer是什么呢?transformer它是一个sequence to sequence model700:00:52,040 --> 00:00:56,360那我们在上课录影里面已经有讲过sequence to sequence model了800:00:56,360 --> 00:01:01,360所以我假设在座各位都知道什么是sequence to sequence model900:01:01,360 --> 00:01:06,000那transformer这种sequence to sequence model它特别的地方是什么呢?1000:01:06,000 --> 00:01:08,000Transformer这个特别的地方1100:01:08,000 --> 00:01:10,000是它在这个Sequence to sequence model里面1200:01:10,000 --> 00:01:12,000大量用到了1300:01:12,000 --> 00:01:14,000Self-attention1400:01:14,000 --> 00:01:16,000这种特别的Layer1500:01:16,000 --> 00:01:18,000接下来我们就是要讲1600:01:18,000 --> 00:01:20,000Self-attention这个Layer1700:01:20,000 --> 00:01:22,000它里面在做的事情是什么1800:01:24,000 --> 00:01:26,500那一般讲到要处理一个sequence1900:01:26,500 --> 00:01:31,640我们最常想到要拿来用的Navigable架构2000:01:31,640 --> 00:01:33,980就是这个RNN2100:01:33,980 --> 00:01:34,780对不对2200:01:34,780 --> 00:01:37,100有时候你会用single direction的2300:01:37,100 --> 00:01:40,740有时候你会用bi-directional的RNN2400:01:40,740 --> 00:01:43,140那RNN基本上它做的事情2500:01:43,140 --> 00:01:45,200因为我们在上课录影里面2600:01:45,200 --> 00:01:47,200已经有跟大家很仔细的讲过了2700:01:47,200 --> 00:01:49,760我们这边就不反复再提2800:01:49,760 --> 00:01:52,480RNN你已经在很多很多地方2900:01:52,480 --> 00:01:55,040都已经有看过这种Navigable架构3000:01:55,040 --> 00:01:58,640RNN的输入就是一串Vector Sequence3100:01:58,640 --> 00:02:03,040它的输出就是另外一串Vector Sequence3200:02:03,040 --> 00:02:07,000今天假设你是Single directional的RN3300:02:07,000 --> 00:02:09,000你在输入B4的时候3400:02:09,000 --> 00:02:13,000你会已经把A1到A4通通都看过3500:02:13,000 --> 00:02:15,000你输出B3的时候3600:02:15,000 --> 00:02:17,000你会把A1到A3都看过3700:02:17,000 --> 00:02:19,000如果你是bi-directional3800:02:19,000 --> 00:02:21,000双向的RN的话3900:02:21,000 --> 00:02:23,000那当你输出这边的4000:02:23,000 --> 00:02:25,040每一个B1到B4的时候4100:02:25,040 --> 00:02:27,040你已经把整个input sequence4200:02:27,040 --> 00:02:30,240通通都看过才输出B1到B44300:02:30,240 --> 00:02:34,040RNN非常常被用在处理input4400:02:34,040 --> 00:02:37,840是有序列的input是一个sequence的状况4500:02:37,840 --> 00:02:40,160但RNN有什么样的问题呢4600:02:40,160 --> 00:02:43,840它的问题就是它不容易被平行化4700:02:43,840 --> 00:02:46,160怎么说它不容易被平行化怎么说它不容易被平行化呢4800:02:46,160 --> 00:02:47,760因为你会发现说4900:02:47,760 --> 00:02:52,240假设你今天要算出b45000:02:52,240 --> 00:02:54,400那你要算出b4怎么做呢5100:02:54,400 --> 00:02:56,480在single direction的情况下5200:02:56,480 --> 00:03:01,280你要先看a1再看a2再看a3再看a45300:03:01,280 --> 00:03:03,760才能够把b4算出来5400:03:03,760 --> 00:03:07,400所以今天这个计算不容易是平行的5500:03:07,400 --> 00:03:11,720也并不容易平行化RNN的运算5600:03:11,720 --> 00:03:13,240所以怎么办呢?5700:03:13,240 --> 00:03:19,880接下来就有人提出了把CNN拿来取代RNN的想法5800:03:19,880 --> 00:03:23,280现在input一样是一个sequence A1到A45900:03:23,280 --> 00:03:25,880但我们不用RNN来处理它6000:03:25,880 --> 00:03:28,720而用CNN来处理它6100:03:28,720 --> 00:03:30,560现在这边每一个三角形呢6200:03:30,560 --> 00:03:33,280就代表一个filter6300:03:33,280 --> 00:03:34,400那这个filter呢6400:03:34,400 --> 00:03:38,600它的输入就是sequence中的其中一小段6500:03:38,600 --> 00:03:40,240比如说现在我们的filter6600:03:40,240 --> 00:03:42,600它就吃三个vector当作输入6700:03:42,600 --> 00:03:45,600然后呢输出一个数值这个filter就把这三个vector当作输入然后呢 输出一个数值6800:03:45,600 --> 00:03:49,200这个filter就把这三个vector里面的内容6900:03:49,200 --> 00:03:52,400串起来 然后跟filter里面的参数做inner product7000:03:52,400 --> 00:03:54,000然后得到一个数值7100:03:54,000 --> 00:03:56,800然后你要把这个filter扫过这个sequence7200:03:56,800 --> 00:03:58,800扫过这个sequence7300:03:59,400 --> 00:04:01,200那filter呢 会不只一个7400:04:01,200 --> 00:04:03,000这边有红色的filter7500:04:03,000 --> 00:04:07,800那你也有黄色的filter它产生另外一排不同的数值7600:04:07,800 --> 00:04:17,000所以用CNN,你确实也可以做到跟RNN的输入跟输出类似的关系,RNN输入是一个Sequence,输出是一个Sequence7700:04:17,000 --> 00:04:25,240用CNN,你有一堆Builder,你也可以做到输入是一个Sequence,输出是另外一个sequence7800:04:27,400 --> 00:04:31,960所以表面上CNN跟RNN他们可以做到有同样的输入跟输出的format7900:04:32,320 --> 00:04:36,160但是现在问题是这边的每一个CNN8000:04:36,320 --> 00:04:39,360他只能够考虑非常有限的内容8100:04:39,400 --> 00:04:40,640这边每一个CNN8200:04:40,680 --> 00:04:42,720他只考虑了三个vector8300:04:42,720 --> 00:04:44,000不像这边的RNN8400:04:44,040 --> 00:04:47,160你可以考虑整个句子才决定你的输出。8500:04:47,520 --> 00:04:57,640但CNN也不是没有办法考虑更长时间的资讯的,它也不是没有办法考虑更长的dependency的。8600:04:58,080 --> 00:05:05,120你只要叠很多层CNN,上层的builder就可以考虑比较多的资讯8700:05:05,120 --> 00:05:08,320举例来说,我们先叠了第一层CNN以后8800:05:08,320 --> 00:05:10,080再叠第二层CNN8900:05:10,080 --> 00:05:15,040第二层CNN的filter会把第一层的output当作它的input9000:05:15,040 --> 00:05:16,560这边这个蓝色的filter9100:05:16,560 --> 00:05:20,480它会看B1 B2 B3来决定它的输出9200:05:20,480 --> 00:05:26,660而B1 B2 B3是根据A1到A4来决定他们的输出9300:05:26,660 --> 00:05:28,660所以等同于是这个蓝色的builder9400:05:28,660 --> 00:05:33,160他其实已经看了整个句子里面所有的内容9500:05:33,160 --> 00:05:35,160所以如果RNN叠很多层9600:05:35,160 --> 00:05:39,160他其实也可以看非常长时间的资讯9700:05:39,160 --> 00:05:41,160而RNN的一个好处是9800:05:41,160 --> 00:05:43,160它是可以平行化的9900:05:43,160 --> 00:05:48,800在这个图片上每一个三角形在做计算的时候10000:05:48,800 --> 00:05:50,440它都可以是平行的10100:05:50,440 --> 00:05:52,360你并不需要等第一个三角形10200:05:52,360 --> 00:05:55,340第一个标准算完才算第二个标准10300:05:55,340 --> 00:05:58,620你可以全部的标准同时计算10400:05:58,620 --> 00:06:01,260你也不需要等红色的标准算完10500:06:01,260 --> 00:06:02,820才算黄色的标准10600:06:02,820 --> 00:06:05,000它们可以同时计算10700:06:05,000 --> 00:06:08,500所以今天CNN是比较容易平行化10800:06:08,500 --> 00:06:16,500但是它有一个缺点就是,今天你一定要叠很多层,它才能够看到长期的资讯10900:06:16,500 --> 00:06:20,500如果今天第一个层的Builder,它就需要长时间的资讯11000:06:20,500 --> 00:06:25,000那做不到,因为它只能够看很小的范围11100:06:25,000 --> 00:06:29,000所以怎么办?有一个新的想法叫做Self-Attention11200:06:29,000 --> 00:06:33,000那Self-Attention他做的事情就是想要取代11300:06:33,000 --> 00:06:36,000那个RNN原来可以做的事情11400:06:36,000 --> 00:06:39,000假设等一下讲的东西你听不下去的话11500:06:39,000 --> 00:06:42,000你就记得今天的关键就是11600:06:42,000 --> 00:06:45,800我们有一种新的Layer叫做Self-Attention Layer11700:06:45,800 --> 00:06:48,440它的输入跟输出跟RNN是一样的11800:06:48,440 --> 00:06:51,120以前RNN是一个sequence output一个sequence11900:06:51,120 --> 00:06:52,800现在有一种新的layer12000:06:52,800 --> 00:06:54,000它不是RNN12100:06:54,000 --> 00:06:56,360但它也可以输入一个sequence12200:06:56,360 --> 00:06:58,120output一个sequence12300:06:58,120 --> 00:06:59,600它特别的地方是12400:06:59,600 --> 00:07:02,840它跟Bidirectional RNN可以有同样的能力12500:07:02,840 --> 00:07:06,080Bidirectional RNN每一个输出都看过整个sequence12600:07:06,080 --> 00:07:13,280Self-attention layer,每个输出,这边的B1到B4,每个输出,它也看过了整个input sequence12700:07:13,280 --> 00:07:19,040但是神奇的地方是,今天B1到B4,它们是同时计算的12800:07:19,040 --> 00:07:24,080它们可以同时被算出来,不需要先算完B1再算B2再算B3才算B412900:07:24,080 --> 00:07:28,000不需要,B1跟B4是同时被计算出来的。13000:07:28,000 --> 00:07:38,000如果等一下讲的东西你听不进去的话,你就记得说,今天我们学到的关键就是,我们可以用Self-Attention Layer来取代RNN。13100:07:38,000 --> 00:07:46,000基本上这个Self-Attention 2017年就已经被proposed出来了,所以今天本来可以用RRN做的东西都已经有人用Self-Attention试过了13200:07:46,000 --> 00:07:48,000都狂洗一轮paper这样子13300:07:48,000 --> 00:07:50,000你可以用RN做的事情13400:07:50,000 --> 00:07:54,000都已经有人用Self-Attention帮你洗过一轮了13500:07:54,000 --> 00:07:58,000今天我已经有点难想到没有用Self-Attention洗过的东西了13600:07:58,000 --> 00:08:00,000好,那Self-Attention这个概念呢13700:08:00,000 --> 00:08:02,000最早出现在一篇paper13800:08:02,000 --> 00:08:04,000这个Google的一篇paper里面13900:08:04,000 --> 00:08:50,860它的title是Attention is all unique的,意思就是说你不需要用RNN,也不需要上一个metric,通过三个不同的factor。14000:08:50,860 --> 00:08:56,200这三个不同的factor,我们分别把它命名为Q、K跟V。14100:08:56,200 --> 00:09:06,720Q代表什么意思呢?等一下我们会更清楚这个Q、K跟V,它们是怎么运作的Q代表的是query它是要去match其他人的14200:09:06,720 --> 00:09:09,200等一下我会讲说所谓的match其他人14300:09:09,200 --> 00:09:10,520是什么意思14400:09:10,520 --> 00:09:12,520你就把每一个input的A14500:09:12,520 --> 00:09:15,720都乘上某一个matrix WQ14600:09:15,720 --> 00:09:18,920然后你就得到Q1到Q414700:09:18,920 --> 00:09:21,520有得到Q1 Q2 Q3跟Q414800:09:21,520 --> 00:09:24,520这些东西叫做query14900:09:24,520 --> 00:09:25,000接下来用同样的方法Q2 Q3 Q4这些东西叫做query15000:09:27,160 --> 00:09:30,200接下来用同样的方法但是你把A乘上不同的match WK15100:09:30,200 --> 00:09:33,200你得到K1到K415200:09:33,200 --> 00:09:35,080这些K叫做key15300:09:35,080 --> 00:09:36,680他们是要拿来被match的15400:09:36,680 --> 00:09:37,960被这个query match15500:09:37,960 --> 00:09:38,880那这个等一下15600:09:38,880 --> 00:09:41,240我们也会再看到说key是怎么用的15700:09:41,840 --> 00:09:44,440然后还有一个东西叫做value15800:09:44,440 --> 00:09:54,000这个VV就是要被抽取出来的information,我们一样把A1到A4存上不同的WV,我们得到V1,V2,V3,V4。15900:09:54,000 --> 00:10:06,440好,那现在呢,我们现在每一个input,每一个A,每一个time step,现在都有QKV三个不同的Vector接下来我们要做的事情是什么呢16000:10:06,440 --> 00:10:08,040接下来要做的事情就是16100:10:08,040 --> 00:10:10,240拿每一个Query Queue16200:10:10,240 --> 00:10:15,680去对每一个TK做Attention这件事情16300:10:15,680 --> 00:10:19,080我们就把每一个Queue去16400:10:19,080 --> 00:10:22,280对每一个K做Attention16500:10:22,280 --> 00:10:24,080我们先把Q1拿出来16600:10:24,080 --> 00:10:25,940等一下每一个Queue都要被用一次从Q1到拿出来等一下每一个Q都要被用一次16700:10:25,940 --> 00:10:28,880从Q1到Q4每一个Q都要被用一次16800:10:28,880 --> 00:10:31,760我们先把Q1拿出来16900:10:31,760 --> 00:10:36,220好 那我们把Q1去对K1做Attention17000:10:36,220 --> 00:10:37,600那等下下一页投影片17100:10:37,600 --> 00:10:39,600会告诉你说Attention的式子17200:10:39,600 --> 00:10:41,240长的是什么样子17300:10:41,240 --> 00:10:43,120那过去我们上课录影17400:10:43,120 --> 00:10:45,000也已经有讲过Attention了17500:10:45,000 --> 00:10:46,000我们说讲Attention17600:10:46,000 --> 00:10:48,000有各式各样不同的算法17700:10:48,000 --> 00:10:49,000它要做的事情17800:10:49,000 --> 00:10:51,000本质上就是吃两个项量17900:10:51,000 --> 00:10:53,000输出就是告诉你说18000:10:53,000 --> 00:10:55,000这两个项量有多匹配18100:10:55,000 --> 00:10:56,000总之吃两个项量18200:10:56,000 --> 00:10:57,000out一个分数18300:10:57,000 --> 00:10:59,000就是Attention做的事情18400:10:59,000 --> 00:11:00,000而至于18500:11:00,000 --> 00:11:01,000怎么吃两个项量18600:11:01,000 --> 00:11:02,000out一个分数18700:11:02,000 --> 00:11:05,560有各式各样不同的做法18800:11:05,560 --> 00:11:07,200等下下一段影片告诉你说18900:11:07,200 --> 00:11:09,680在Google的Self-Attention的Layer里面19000:11:09,680 --> 00:11:11,920他们做的事情是什么19100:11:11,920 --> 00:11:15,200总之我们把Q1跟K1做Attention19200:11:15,200 --> 00:11:17,200比如说你可以做Inner Product19300:11:17,200 --> 00:11:18,680你就得到一个数值19400:11:18,680 --> 00:11:23,040代表Q1跟K1之间的Attention的Weight19500:11:23,040 --> 00:11:24,440我们叫做α万万19600:11:24,440 --> 00:11:28,000代表1 Q1对之间的Attention的位置我们叫做α1-1代表1 Q1对K1的Attention19700:11:28,800 --> 00:11:29,800然后接下来呢19800:11:29,800 --> 00:11:33,000你拿Q1也对K2做Attention19900:11:33,000 --> 00:11:34,600得到α1-220000:11:34,600 --> 00:11:37,700你拿Q1对K3也做Attention20100:11:37,700 --> 00:11:39,200得到α1-320200:11:39,200 --> 00:11:41,800你拿Q1对K4也做Attention20300:11:41,800 --> 00:11:44,000得到α1-420400:11:44,800 --> 00:11:46,500那这个Attention怎么算的呢?20500:11:46,500 --> 00:11:50,500在这个Scalable Attention Layer里面用的叫做20600:11:50,500 --> 00:11:53,500Scaled Top Product Attention20700:11:53,500 --> 00:11:58,000也就是说,这下面的αEi是怎么来的呢?20800:11:58,000 --> 00:12:03,500就是拿Q1对Ki做Inner Product20900:12:03,500 --> 00:12:06,000然后再除以√D这边比较难理解的是,然后再除以根号D21000:12:06,000 --> 00:12:08,000这边比较难理解的是21100:12:08,000 --> 00:12:10,000为什么要除以根号D呢?D是什么?21200:12:10,000 --> 00:12:12,000D是Q跟K的dimension21300:12:12,000 --> 00:12:14,000那这边因为你跟Q跟K21400:12:14,000 --> 00:12:16,000要做inner product21500:12:16,000 --> 00:12:18,000所以Q跟K的dimension会是一样的21600:12:18,000 --> 00:12:20,000这个D呢就是Q跟K的dimension21700:12:20,000 --> 00:12:22,000那你要把Q跟K21800:12:22,000 --> 00:12:24,000做inner product以后21900:12:24,000 --> 00:12:28,920再除以根号D那这个为什么要除以这个d呢?22000:12:28,920 --> 00:13:05,760这边一个直观的解释是说,你知道Q跟K做double product的数值,会随着它们的dimension越大,它的variance就越大,这个很直观嘛,就是Q跟K的dimension越大,那你相乘的时候,你这个做double order的时候,相加的element就越多,所以balance就越大,所以我们除以�脚来解释这件事啦,那你自己再看看。22100:13:05,760 --> 00:13:25,400那我没有做过实验说除根号D,有没有除根号D到底影响有多大,原始的论文应该也没有做这个实验,那如果有同学有做这个实验的话,你再来告诉我说这件事情影响有多大,我明年就把你的名字加到这个投影片上面这样子好22200:13:25,400 --> 00:13:26,600那22300:13:26,600 --> 00:13:32,200总之我觉得这边你也不一定要用Scalable Data的Attention的方法22400:13:32,200 --> 00:13:33,200有很多种22500:13:33,200 --> 00:13:34,800总之只要吃两个Meta22600:13:34,800 --> 00:13:36,000套出一个分数22700:13:36,000 --> 00:13:37,200应该就可以了22800:13:37,200 --> 00:13:38,800但是我其实不知道说22900:13:38,800 --> 00:13:42,000因为我们一般现在在使用这个Scalable Attention Layer的时候23000:13:42,000 --> 00:13:44,600就直接套用原始Paper的做法23100:13:44,600 --> 00:13:46,700也不会做太多的更动那我其实不知道说如果把Delf-Attention Layer的时候就直接套用原始Paper的做法也不会做太多的更动23200:13:46,700 --> 00:13:49,360那我其实不知道说如果把Dark Wadah的Attention23300:13:49,360 --> 00:13:51,360换成其他种类的Attention23400:13:51,360 --> 00:13:52,660会有多大的影响23500:13:52,660 --> 00:13:54,660那这边都是按照原始Paper的讲法23600:13:54,660 --> 00:13:56,660来跟大家陈述一下23700:13:56,660 --> 00:13:58,660好那23800:13:58,660 --> 00:13:59,860接下来呢23900:13:59,860 --> 00:14:02,460你会做一个东西叫做Solve-Max24000:14:02,460 --> 00:14:28,360也就是你会把Alpha 1-1到Alphasoftmax的layer,得到α11 hat到α14 hat。Softmax做的事情就是把这边的每一个α都取exponential然后再除掉所有α取exponential的总和24100:14:28,360 --> 00:14:31,360做一下normalization就得到α hat24200:14:32,560 --> 00:14:34,120好 做完Softmax以后24300:14:34,120 --> 00:14:36,120接下来我们有了α hat以后24400:14:36,120 --> 00:14:43,200我们拿α hat去跟每一个V去相乘24500:14:43,200 --> 00:14:46,680我们就把α 1 1 hat乘上V1把把α11 hat乘上V124600:14:46,680 --> 00:14:48,680把α12 hat乘上V224700:14:48,680 --> 00:14:50,680把α13 hat乘上V324800:14:50,680 --> 00:14:52,680把α14 hat乘上V424900:14:52,680 --> 00:14:56,280这边做的事情等于就是把V1到V425000:14:56,280 --> 00:14:58,280用α1 hat到α1425100:14:58,280 --> 00:15:00,280做weighted sum25200:15:00,280 --> 00:15:03,080你把所有的Vi乘上α1i hat25300:15:03,080 --> 00:15:04,400再通通加起来25400:15:04,400 --> 00:15:08,200你就得到一个vector,这个vector就是V125500:15:08,200 --> 00:15:12,120我们刚才讲说self-attention就是输出一个sequence25600:15:12,120 --> 00:15:13,960输入一个sequence,输出也是一个sequence25700:15:13,960 --> 00:15:20,480那我们现在已经得到我们要输出的那个sequence的第一个vector就是V125800:15:20,480 --> 00:15:25,120但是你会发现说今天我们在产生这个B1的时候25900:15:25,120 --> 00:15:28,560它其实用了整个sequence的资讯26000:15:28,560 --> 00:15:30,160因为B1是哪来?26100:15:30,160 --> 00:15:32,720它是B1到B426200:15:32,720 --> 00:15:35,280做weighted sum得到26300:15:35,280 --> 00:15:38,400而B1到B4又是A126400:15:38,400 --> 00:15:41,840从A1到A4做一个transformation得到26500:15:41,840 --> 00:15:44,080所以等于是你产生B1的时候26600:15:44,080 --> 00:15:48,000你已经看到了A1到A4之间的资讯26700:15:48,000 --> 00:15:50,000如果你今天产生B1的时候26800:15:50,000 --> 00:15:52,000你不想考虑整个句子的资讯26900:15:52,000 --> 00:15:54,000只想考虑local的information27000:15:54,000 --> 00:15:56,000对self-attainment layer来说27100:15:56,000 --> 00:15:57,000也是做得到的27200:15:57,000 --> 00:16:01,000他只要让这边的alpha产生出来的值变成027300:16:01,000 --> 00:16:04,000那他就考虑local的information27400:16:04,000 --> 00:16:12,360如果他要考虑global的information他要考虑跟它最远的这个input,跟它最远的这个vector,它的资讯的话,27500:16:12,360 --> 00:16:37,340它只要把这个attention让它有值,他就可以用Attention去把再远的vector,他的这个information都可以用Attention被撷取出来。27600:16:37,340 --> 00:16:47,200那至于要看近看远,要看哪一个invisible的哪一个部分,用Attention,自己用学的来决定所以这就是Sale Attention的妙用27700:16:49,200 --> 00:16:51,920好那刚才算出来的是B127800:16:51,920 --> 00:16:54,080那其实今天在同一时间27900:16:54,080 --> 00:16:55,760你也可以算B228000:16:55,840 --> 00:16:58,240在output sequence里面的每一个B28100:16:58,240 --> 00:17:01,200他们计算出来都是这个平行的28200:17:01,200 --> 00:17:02,880他们是可以同时做运算的28300:17:02,880 --> 00:17:04,319这边算出B128400:17:04,400 --> 00:17:06,200同时呢也算B2算出B1同时也算B228500:17:06,200 --> 00:17:08,680B2就是拿Query Q228600:17:08,680 --> 00:17:11,079去对其他的K做Attention28700:17:11,079 --> 00:17:13,800所以我们就把Q2对K128800:17:13,800 --> 00:17:15,480算一下它的Attention28900:17:15,480 --> 00:17:17,720Q2对K2做Attention29000:17:17,720 --> 00:17:19,280Q2对K3做Attention29100:17:19,280 --> 00:17:20,839Q2对K4做Attention29200:17:20,839 --> 00:17:22,079然后你要做SortMath29300:17:22,079 --> 00:17:23,680得到一串AlphaHat29400:17:23,680 --> 00:17:27,760接下来再拿这些AlphaH hat对input的b呢29500:17:27,760 --> 00:17:28,960做weighted sum29600:17:28,960 --> 00:17:31,960然后你就把alpha21 hat乘v129700:17:31,960 --> 00:17:33,800alpha22 hat乘v229800:17:33,800 --> 00:17:35,440alpha23 hat乘v329900:17:35,440 --> 00:17:37,160alpha24 hat乘v430000:17:37,160 --> 00:17:39,080把它们全部加起来30100:17:39,080 --> 00:17:41,640得到b2就结束了30200:17:41,640 --> 00:17:43,320所以现在我们就算出了30300:17:43,320 --> 00:17:46,000output sequence的第二个vector30400:17:46,000 --> 00:17:48,000就是D230500:17:48,000 --> 00:17:50,000用同样的方法30600:17:50,000 --> 00:17:52,000就可以把D3跟D430700:17:52,000 --> 00:17:54,000都算出来30800:17:54,000 --> 00:17:56,000那如果讲刚才讲的东西30900:17:56,000 --> 00:17:57,000你没有跟上的话31000:17:57,000 --> 00:17:58,000你就记得说31100:17:58,000 --> 00:17:59,000现在input的一个sequence31200:17:59,000 --> 00:18:00,000A1到A431300:18:00,000 --> 00:18:02,000我们得到了另外一个sequence31400:18:02,000 --> 00:18:04,000D1到D431500:18:04,000 --> 00:18:05,280所以我们Pension Layer它做的事情31600:18:05,280 --> 00:18:07,680跟RNN是一样的31700:18:07,680 --> 00:18:08,960只是B1到B431800:18:08,960 --> 00:18:13,600它们可以平行的被计算出来31900:18:13,600 --> 00:18:14,880好 那接下来呢32000:18:14,880 --> 00:18:18,720我们要把刚才的一连串的运算过程32100:18:18,720 --> 00:18:21,360Self-Attention里面的一连串的运算过程32200:18:21,360 --> 00:18:23,120用矩阵运算来表示32300:18:23,120 --> 00:18:24,800这样你就会更有感觉说32400:18:24,800 --> 00:18:27,800为什么刚才那一连串的运算过程用矩阵运算来表示这样你就会更有感觉说为什么刚才那一连串的运算过程32500:18:27,800 --> 00:18:29,400是容易被平行化32600:18:29,400 --> 00:18:31,240容易被加速的32700:18:31,240 --> 00:18:32,520好 接下来呢32800:18:32,520 --> 00:18:36,120我们要再跟大家更清楚的说明说32900:18:36,120 --> 00:18:37,400Self-Attention33000:18:37,400 --> 00:18:40,600是怎么做平行化的33100:18:40,600 --> 00:18:42,600好 那我们刚才有说呢33200:18:42,600 --> 00:18:47,280A1乘以一个matrix WQ 就会变成Q133300:18:47,280 --> 00:18:52,000A2再乘以WQ 就会变成Q233400:18:52,000 --> 00:18:56,400A3再乘以WQ 也会变成Q333500:18:56,400 --> 00:18:58,760A4除以WQ 会变成Q433600:18:58,760 --> 00:19:02,720所以你可以把A1到A4拼起来33700:19:02,720 --> 00:19:04,080变成一个matrix33800:19:04,080 --> 00:19:07,920这个matrix呢 我们这边用大I来表示33900:19:07,920 --> 00:19:14,460我们用大I乘上WQ一次就可以得到matrix Q34000:19:14,460 --> 00:19:18,160matrix Q里面的每一个column就代表一个小Q34100:19:18,160 --> 00:19:21,760就代表某一个position的query34200:19:21,760 --> 00:19:29,000同理我们把A1到A4串起来 乘上WK 就可以得到每个position的key34300:19:29,000 --> 00:19:38,000我们把I乘上WK 就得到matrix K 大K的每一个column 就是每一个位置的key34400:19:38,000 --> 00:19:54,060对value来说也是一样 把所有的A串起来变成I,I乘以WV,就得到所有position的value,这边用V来表示。34500:19:54,060 --> 00:20:06,820接下来,我们刚才说,我们拿Q1去对每一个K做attention,其实就是做double product这边我们省略了根号K这样子可以让notation更简洁一点34600:20:06,820 --> 00:20:10,260我们把K1跟Q1做double product34700:20:10,260 --> 00:20:12,260所谓K1跟Q1做double product的意思34800:20:12,260 --> 00:20:14,920就是把K1呢做一个transpose34900:20:14,920 --> 00:20:16,920就是我们现在把K1放倒下来35000:20:16,920 --> 00:20:18,920啊它倒下来了,它倒下来它做transpose35100:20:18,920 --> 00:20:21,420跟Q1呢做double product35200:20:21,420 --> 00:20:23,420得到α1135300:20:23,420 --> 00:20:27,000这个K2跟Q2做double product得到α12K2跟Q2做double order 得到α1235400:20:27,000 --> 00:20:30,000K3跟Q1做double order 得到α1335500:20:30,000 --> 00:20:33,000K4跟Q1做double order 得到α1435600:20:33,000 --> 00:20:34,000那你会发现说呢35700:20:34,000 --> 00:20:37,000当我们计算α11到α14的时候35800:20:37,000 --> 00:20:39,000其实都是拿Q1来做计算35900:20:39,000 --> 00:20:44,000只是跟不同的K跟K1到K4来做double order36000:20:44,000 --> 00:20:46,300所以我们现在可以把所有的K36100:20:46,300 --> 00:20:47,900统统集合在一起36200:20:47,900 --> 00:20:52,020你把所有的K当作是某一个matrix的row36300:20:52,020 --> 00:20:55,220所以把K1到K4统统叠在一起36400:20:55,220 --> 00:20:56,660变成一个matrix36500:20:56,660 --> 00:20:58,860直接把这个matrix乘上Q136600:20:58,860 --> 00:21:01,940你得到的结果就是一个向量36700:21:01,940 --> 00:21:06,000那这个向量里面的值就是α11到α1436800:21:07,000 --> 00:21:12,000所以α11到α14的计算其实也是可以平行的36900:21:12,000 --> 00:21:14,000然后接下来呢37000:21:14,000 --> 00:21:16,000你还有其他的Q37100:21:16,000 --> 00:21:18,000我等下把Q2拿出来37200:21:18,000 --> 00:21:22,000其实Q2做的事情也是一样跟K1到K4做double quadratum算作的配置37300:21:22,000 --> 00:21:27,240所以就把Q2跟这一个由K所组成的matrix37400:21:27,240 --> 00:21:29,160做一下double quadrat37500:21:29,160 --> 00:21:30,800然后做一下相乘37600:21:30,800 --> 00:21:33,440然后你就得到αQ1到αQ437700:21:33,440 --> 00:21:37,960同理Q3拿出来跟这个matrix做个相乘37800:21:37,960 --> 00:21:40,320得到α31到α3437900:21:40,320 --> 00:21:41,680Q4你拿出来做个相乘38000:21:41,680 --> 00:21:44,320得到α41到α4438100:21:44,320 --> 00:21:46,680所以今天这整个运算的过程38200:21:46,680 --> 00:21:49,480这整个计算出Attention的过程38300:21:49,480 --> 00:21:54,160其实就等于是把我们得到的matrix K38400:21:54,160 --> 00:21:57,200做一个transpose直接乘上Q38500:21:57,200 --> 00:21:59,560就得到Attention A38600:21:59,560 --> 00:22:03,280每一个time step两两之间都有Attention38700:22:03,280 --> 00:22:06,440那这边我们如果有四个input38800:22:06,440 --> 00:22:08,680那这个attention的matrix就是这四38900:22:08,680 --> 00:22:11,000你input sequence长度如果是N39000:22:11,000 --> 00:22:16,040那这个attention它就变成一个matrix是N乘以N39100:22:16,040 --> 00:22:18,560接下来你会做solvemax得到alpha hat39200:22:18,560 --> 00:22:19,560这就没有什么好说的39300:22:19,560 --> 00:22:22,360你就对每一个column做一下solvemax39400:22:22,360 --> 00:22:25,280把A变成A hat就结束了39500:22:25,280 --> 00:22:29,120把matrix A变成matrix A hat就结束了39600:22:30,520 --> 00:22:35,360好那接下来我们要把这些V1到V439700:22:35,360 --> 00:22:38,720根据alpha hat做weighted sum39800:22:38,720 --> 00:22:40,480那怎么做weighted sum呢39900:22:40,480 --> 00:22:44,400我们把我们刚才计算出来的matrix B拿出来40000:22:44,400 --> 00:22:51,500matrix B里面的每一个color都是一个value的vector小v40100:22:51,500 --> 00:22:54,900那怎么计算B1呢40200:22:54,900 --> 00:22:58,140我们就把α11 hat乘上B140300:22:58,140 --> 00:23:00,140把α12 hat乘上B240400:23:00,140 --> 00:23:02,140把α13 hat乘上B340500:23:02,140 --> 00:23:04,140把α14 hat乘上B440600:23:04,140 --> 00:23:07,140就是把B1到B4对α11到α1440700:23:07,140 --> 00:23:09,340做微计上就得到B140800:23:09,340 --> 00:23:11,340那这个就是简单的线性代数40900:23:11,340 --> 00:23:13,340我觉得对大家来说应该都非常熟悉41000:23:13,340 --> 00:23:18,700你把这个matrix A head的第二个column拿出来41100:23:18,700 --> 00:23:22,340对B做相乘你就得到B241200:23:22,340 --> 00:23:30,800你把第三个column拿出来对B做相乘就得到B2,你把第三个column拿出来对B做相乘就得到B3,你把最后一个column拿出来对B做相乘就得到B441300:23:30,800 --> 00:23:36,960那B1到B4串起来就是最终整个self-attention的输出我们叫它O41400:23:36,960 --> 00:23:47,180所以最后怎么得到self-attention的输出呢?你就把A hat乘上B这两个取证相乘�就得到O最后Layer的输出就结束了41500:23:48,740 --> 00:23:51,940好 那我们再把刚才的整个运算41600:23:51,940 --> 00:23:53,780再很快的看过一次41700:23:53,780 --> 00:23:55,980整个Self-Attention Layer它的输入41800:23:55,980 --> 00:23:58,100是一个Matrix我们叫做I41900:23:58,100 --> 00:24:01,180输出是一个Matrix叫做O42000:24:01,180 --> 00:24:03,340接下来我们要看看42100:24:03,340 --> 00:24:05,720从Input的matrix I42200:24:05,720 --> 00:24:08,000到output的matrix O42300:24:08,000 --> 00:24:09,000这中间42400:24:09,000 --> 00:24:10,920SELF ATTENTION的layer42500:24:10,920 --> 00:24:14,240做了哪些事情42600:24:14,240 --> 00:24:16,360首先input的matrix I42700:24:16,360 --> 00:24:19,440会分别乘上三个不同的matrix42800:24:19,440 --> 00:24:23,200WQ WK 跟 WB42900:24:23,200 --> 00:24:26,000然后你就得到一个matrix Q43000:24:26,000 --> 00:24:29,000matrix K 跟 matrix V43100:24:29,000 --> 00:24:32,000这三个matrix 他们的每一个column43200:24:32,000 --> 00:24:35,000就分别代表了某一个位置的43300:24:35,000 --> 00:24:38,000query key跟value43400:24:38,000 --> 00:24:41,000接下来你把K做transpose43500:24:41,000 --> 00:24:44,000乘上Q 就会得到43600:24:44,000 --> 00:24:48,600attention的mat matrix 这边用大 A 来表示43700:24:48,600 --> 00:24:51,600这个 Attention matrix 里面每一个 element43800:24:51,600 --> 00:24:55,440就代表了现在 input 的这个 sequence43900:24:55,440 --> 00:24:59,080这个 input 的 sequence 两两之间44000:24:59,080 --> 00:25:04,520input 的 sequence 每一个位置两两之间的 attention44100:25:04,520 --> 00:25:08,440A 接下来做 softftmax得到Ahat44200:25:08,440 --> 00:25:13,960然后呢 再把Ahat乘上V就得到最终的O44300:25:13,960 --> 00:25:17,040所以你要发现在Self-Attention Layer里面44400:25:17,040 --> 00:25:20,960我们做的就是一连串的矩阵乘法44500:25:20,960 --> 00:25:25,760而矩阵乘法可以轻易的用GPU来加速44600:25:26,760 --> 00:25:31,520好,那这个Self-Attention有一个变形叫做Multi-Hat的Self-Attention44700:25:31,520 --> 00:25:33,920什么叫Multi-Hat的Self-Attention呢?44800:25:33,920 --> 00:25:38,520我们这边用有两个Hat的情况来做举例44900:25:39,120 --> 00:25:44,040好,那我们刚才说每一个AI都会得到QI、KI跟VI45000:25:44,400 --> 00:25:46,160这是我们刚才讲的例子45100:25:46,160 --> 00:25:48,160在有两个head的情况下45200:25:48,160 --> 00:25:54,560你会进一步把Q再把它分裂变成有两个Q45300:25:54,560 --> 00:25:56,560那你这边的做法可以是45400:25:56,560 --> 00:26:00,160我把QI乘上一个matrix变成QI145500:26:00,160 --> 00:26:03,360我把QI乘上一个matrix变成QI245600:26:03,360 --> 00:26:26,520我把QI乘上两个不同的matrix分别变成QI1跟QIQ,那K也一样,K也产生对KI1、KJ145700:26:26,520 --> 00:26:32,640他只会对跟他同样是第一个的vector做attention45800:26:32,640 --> 00:26:37,200举例来说,假设你要拿QI1去跟别人做attention45900:26:37,200 --> 00:26:42,960你算大发达的key,你只会跟KI1跟KJ1去做计算46000:26:42,960 --> 00:26:46,500你得到你的attention,然后你最后计算出BI1跟KJ1去做计算你得到你的Attention然后你最后计算出BI146100:26:47,200 --> 00:26:48,800那QI2也是一样46200:26:48,800 --> 00:26:49,700QI2呢46300:26:49,700 --> 00:26:53,100它只跟KI2、KJ2去做Attention46400:26:53,100 --> 00:26:55,100得到BI246500:26:55,500 --> 00:26:58,500那你会把BI1跟BI2直接concatenate起来46600:26:58,700 --> 00:27:00,700那如果concatenate完以后46700:27:00,700 --> 00:27:03,300你的这个Dimension你不喜欢的话46800:27:03,500 --> 00:27:16,220你会再乘上一个Transform,把它做一下相尾,得到VI,VI1跟VI2,接起来乘上WO,就得到VI,这个就是你的Self-Attention Layer最终的输出。46900:27:16,220 --> 00:27:34,200那这个做这个Multi-Head有什么好处呢?有可能不同的Head,等一下我们会看到说原始paper里面的例子,有可能不同的APP,他们关注的点就是不一样。举例来说,可能有的APP想要看的就是Local的资讯,看的就是邻居的资讯。47000:27:34,200 --> 00:27:46,080有的APP想要看的是比较长时间的资讯,想要看的是比较Global的资讯。你有了Multi-Head以后,每个APP可能就可以各司其职,自己去做自己想做的事情。47100:27:46,080 --> 00:27:55,880这是Multi-head的SELF-ATTENTION,这边是用两个head做例子,在实作的时候,head的数目也是一个参数要调的,你可以做8个head,10个head等等都可以。47200:28:15,640 --> 00:28:47,700如果你仔细看一下,讲一下这个Self-Attention的Layer的话,你会发现说对Self-Attention来说,Input的那个Sequence的顺序是不重要的。为什么?对它来说,它做的事情就是跟每一个Input的Vector都做self-attention,都做attention,所以对每一个时间点来说,跟他是邻居的东西,还是跟他是在天涯的东西,对他来说都是一样的,对他来说这个就是天涯落壁,对他来说邻居跟天涯其实是一样的,所以在他心里并没有所谓的未置的资讯所以在Self-Attention Layer来说如果你Input A打了B47300:28:47,700 --> 00:28:49,700跟B打了A是一样的47400:28:49,700 --> 00:28:52,900因为它并不考虑这个Input Sequence的顺序47500:28:52,900 --> 00:28:55,700而这个显然不是我们要的47600:28:55,700 --> 00:28:58,700我们希望能够把Input Sequence的顺序47700:28:58,700 --> 00:29:02,400考虑进去Self-Attention Layer里面47800:29:02,400 --> 00:29:04,900那所以在原始的Paper它是怎么说的呢?47900:29:04,900 --> 00:29:06,000它说现在你把SSelf-Attention Layer里面那所以在原始的paper他是怎么说的呢?他说48000:29:06,000 --> 00:29:08,000现在啊,你把48100:29:08,000 --> 00:29:10,000Input Si 通过一个全通48200:29:10,000 --> 00:29:12,000变成 Ai 以后48300:29:12,000 --> 00:29:14,000他还要再加上一个48400:29:14,000 --> 00:29:16,000Ei48500:29:16,000 --> 00:29:18,000再加上一个神奇的Vector48600:29:18,000 --> 00:29:20,000叫做Ei,但Ei要跟48700:29:20,000 --> 00:29:22,000Ai的Dimension是一样的啦48800:29:22,000 --> 00:29:24,000Dimension是一样的才能够加上去48900:29:24,000 --> 00:29:25,160好,那这个Ei他是首设的它的dimension是一样的啦它的dimension是一样的才能够加上去49000:29:26,360 --> 00:29:28,080好,那这个EI它是手设的49100:29:28,080 --> 00:29:28,960它是Handcrafted的49200:29:28,960 --> 00:29:30,440在原始的FILM49300:29:30,440 --> 00:29:31,640就是Attention is All You Need49400:29:31,640 --> 00:29:32,880那篇paper里面49500:29:32,880 --> 00:29:35,560EI不是学出来的49600:29:35,560 --> 00:29:38,160是直接人手设的49700:29:38,160 --> 00:29:40,600而这个EI代表了位置的资讯49800:29:40,600 --> 00:29:41,520每一个资讯49900:29:41,520 --> 00:29:44,800每一个位置都有一个不同的EI50000:29:44,800 --> 00:29:46,560就是第一个位置有一个E1第二个位置有一个E2第三一个位置都有一个不同的AI就是第一个位置有一个E150100:29:46,560 --> 00:29:47,800第二个位置有一个E250200:29:47,800 --> 00:29:49,000第三个位置有一个E350300:29:49,000 --> 00:29:51,000你只要看这个E长什么样子50400:29:51,000 --> 00:29:52,440看这个vector长什么样子50500:29:52,440 --> 00:29:54,840就知道说现在的AI50600:29:54,840 --> 00:29:58,400它的资讯它是在第几个position50700:29:58,400 --> 00:29:59,960好,所以这边原来的PayPal50800:29:59,960 --> 00:30:02,840它做的事情就是把EI加到AI里面50900:30:02,840 --> 00:30:04,240得到一个新的vector51000:30:04,240 --> 00:30:10,660接下来就没有什么不同就直接apply一模一样的self-attention的operation。51100:30:11,420 --> 00:30:21,100那讲到这边,通常大家会有的一个问题啊,就是为什么是相加,为什么不是concatenate?51200:30:54,700 --> 00:30:58,580concatenate,你把EI跟AI加起来,那个原来位置的资讯不就混到AI里面去了吗?不就很难被找到了一样。我们这边可以想像说,我们把 input Xi 再贴一个,再 append,再 concatenate,一个 one-hot vector 叫做 pi,这个 pi 代表了未知的资讯。51300:30:58,580 --> 00:31:08,240今天 pi 就是一个很长的 vector,这个 vector 是 one-ctor,只有一维是1,其他都是0那pi的话就是第i维是1,其他都是051400:31:08,240 --> 00:31:11,840如果是p1就是第1维就是1,其他都是051500:31:11,840 --> 00:31:14,240如果是p2就是第2维是1,其他都是051600:31:14,240 --> 00:31:18,640一看这个1号vector就知道这个vector是落在哪一个位置51700:31:18,640 --> 00:31:23,360它要跟xi的哪一个x做concatenate51800:31:23,360 --> 00:31:27,680好,那我们现在把xi跟pi做concatenate好,那我们现在把SI跟PI做concatenate51900:31:27,680 --> 00:31:30,400然后把它乘上一个matrix52000:31:30,400 --> 00:31:33,200让它的embedding做一个transform52100:31:33,600 --> 00:31:36,160那你可以想像说这个W52200:31:36,160 --> 00:31:37,440可以拆成两个vector52300:31:37,440 --> 00:31:40,240我们这边写成Wi跟Wp52400:31:40,240 --> 00:31:42,400那就把Wi跟SI相乘52500:31:42,400 --> 00:31:44,720Wp跟PI相乘52600:31:44,720 --> 00:31:47,600那这个就是一般的如果你有学线性代数的话52700:31:47,600 --> 00:31:51,600就是Metric Partition的概念,这个没有什么特别的52800:31:51,600 --> 00:31:55,600所以这件事情就把Xi跟Pi串起来52900:31:55,600 --> 00:31:57,800再乘上一个Metric这件事情53000:31:57,800 --> 00:32:01,200等于把Xi乘上一个Metric Wi53100:32:01,200 --> 00:32:04,400加上Pi乘上一个Metric Wp53200:32:04,400 --> 00:32:09,600而这个部分,Xi乘上一个 metric wi 加上 pi 乘上一个 metric wp而这个部分,xi 乘上 wi 的部分就是 ai53300:32:09,600 --> 00:32:13,200那把 pi 乘上 wp 的部分就是 ai53400:32:13,200 --> 00:32:15,200然后把 ai 跟 ei 加起来53500:32:15,200 --> 00:32:18,000等同于我们把原来 input x53600:32:18,000 --> 00:32:20,400append 一个 one-hot 的资讯53700:32:20,400 --> 00:32:22,400再做一个 transformation53800:32:22,400 --> 00:32:29,380所以我觉得直接把 ei 加到 ai 里面并没有什么特别奇怪的地方是可以说得通的。53900:32:29,380 --> 00:32:52,420这边让人匪夷所思的地方就是,你当然可以认这个WP说,WP论的这件事情,当然有人做过了,之前做convolution的这个sequence to sequence model的时候就有人尝试过类似的做法,让他们试一下,没有比较好。54000:32:52,420 --> 00:33:05,800然后他们WP是怎么来的?他们WP是人手设的,他们有一个非常奇怪的式子可以去产生那个WP,这个WP这个metric就长这样子啦就长右边这样54100:33:05,800 --> 00:33:09,100这个图我相信你一定在网络上很多地方都看过了54200:33:09,100 --> 00:33:12,600很多人都已经试着把那个WP画出来54300:33:12,600 --> 00:33:15,300看看它长什么样子54400:33:15,300 --> 00:33:16,800所以这个WP54500:33:16,800 --> 00:33:19,600它就长这个样子54600:33:19,600 --> 00:33:22,600它就长这个样子54700:33:22,600 --> 00:33:45,440那那为什么在你的问题字加在投团上分享这样子如果你有好的想法我明年用你的讲法的话54800:33:45,440 --> 00:33:47,640那我就把你的名字加在投影片上54900:33:48,600 --> 00:33:50,520刚才讲的是说55000:33:50,520 --> 00:33:52,520我们可以把Self-Attention55100:33:52,520 --> 00:33:55,040拿来取代一个RN55200:33:55,440 --> 00:33:57,560接下来我们继续看说55300:33:57,560 --> 00:33:58,920Self-Attention在一个55400:33:58,920 --> 00:34:01,240Sequence to Sequence的Model里面55500:34:01,600 --> 00:34:03,240它是怎么被使用55600:34:03,760 --> 00:34:45,120那一般的Sequence to Sequence Model一般用RN做的Sequence的model里面,它是怎么被使用?那一般的Sequence-to-Sequence model,一般用RNN做的Sequence-to-Sequence model,我们已经讲过了,我们已经讲过了,我们说Sequence-to-Sequence model里面有两个RNN, X1到H455700:34:45,120 --> 00:34:48,760那这个Bidirectional RNN现在你已经知道55800:34:48,760 --> 00:34:52,320可以用Cell Attention把它取代掉55900:34:52,320 --> 00:34:53,400如果你不喜欢RNN56000:34:53,400 --> 00:34:55,880觉得它运算太慢无法平行化的话56100:34:55,880 --> 00:34:58,800用Cell Attention来取代它56200:34:58,800 --> 00:35:01,560Decoder的部分也是一个RNN56300:35:01,560 --> 00:35:03,160原来我们做的事情是56400:35:03,160 --> 00:35:06,160我们会做Attention56500:35:06,160 --> 00:35:09,240从每一个Decoder TimeState56600:35:09,240 --> 00:35:12,400你都会根据之前Encoder的Output56700:35:12,400 --> 00:35:13,560做一下Attention56800:35:13,560 --> 00:35:15,560每一个TimeState都有一个Attention56900:35:15,560 --> 00:35:18,280然后你每一个TimeState都会输出的东西57000:35:18,280 --> 00:35:20,720这也是一个RNN57100:35:20,720 --> 00:35:22,960这个RNN也可以用57200:35:22,960 --> 00:35:24,560Self-Attention的Layer57300:35:24,560 --> 00:35:34,700把它取代掉简单的概念就是这个样子总之看到RNN也可以用Self-Attention的Layer取代掉,简单的概念就是这个样子,总之看到RNN都用Self-Attention把它换掉就结束了。57400:36:05,520 --> 00:36:06,720Google的blog上面的动画啦,那现在Google的blog上面都会有一些很清楚的动画,那这个动画就是告诉你说这个,如果我们今天用都做attention好 从下一个time state起57500:36:06,720 --> 00:36:09,640他会告诉你说这些attention是平行运算的57600:36:10,040 --> 00:36:12,640是平行运算的 每一层都做self-attention57700:36:13,240 --> 00:36:14,280好 接下来decode了57800:36:14,640 --> 00:36:15,880接下来decode decode的时候57900:36:16,200 --> 00:36:18,640会对input的encoder做attention58000:36:20,040 --> 00:36:21,320好 那但是在这边58100:36:21,640 --> 00:36:24,000在decode第二个word的时候58200:36:24,240 --> 00:36:25,800他不只是对 input 做 attention,58300:36:25,800 --> 00:36:29,500它也会对之前已经产生出来的东西做 attention。58400:36:31,900 --> 00:36:34,800最后就产生出一个句子,58500:36:34,800 --> 00:36:38,900input 是英文, output 是另外一个语言的翻译结果。58600:36:38,900 --> 00:36:42,000我们再看一次,现在是 encode 的状况。58700:36:43,100 --> 00:36:46,000encode 的时候,所有 input sequence 的 word,两两之间都要做 attention,现在是Encode的状况Encode的时候所有Input的Sequence的Word58800:36:46,000 --> 00:36:48,000两两之间都要做Attention58900:36:48,000 --> 00:36:50,000这些Attention是平行的59000:36:50,000 --> 00:36:52,000做三次59100:36:52,000 --> 00:36:54,000有三个Attention的Layer59200:36:54,000 --> 00:36:56,000接下来做Decode59300:36:58,000 --> 00:37:00,000在做Decode的时候59400:37:00,000 --> 00:37:02,000不只会Attent之前的Input59500:37:02,000 --> 00:37:04,000也会Attent之前已经输出的部分59600:37:06,780 --> 00:37:09,080也会Attent之前已经输出的部分。也会Attent之前已经输出的部分,看到吗?59700:37:09,080 --> 00:37:10,860他会Attent之前已经输出的部分。59800:37:12,680 --> 00:37:17,200之前已经输出三个word,所以这三个word都有Attent,都有射东西过来。59900:37:18,120 --> 00:37:24,600就这样,所以这个就是拿Self-Attention做在Sequence-to-Sequence Model上的样子。60000:37:25,960 --> 00:37:28,760那接下来呢,就很快的看一下这个内网的架构。60100:37:28,760 --> 00:37:32,200这个图,你一定看到不想再看了对不对?60200:37:32,200 --> 00:37:43,760这个图出现在各式各样的地方,就通常在讲Transformer的时候,就会跟你说,Transformer就是长这样啦,不懂的人你还是看不懂啦,就是这种事。60300:37:43,760 --> 00:37:48,000现在我们来看一下这个图里面有什么样的东西。60400:37:48,000 --> 00:38:12,000这个图讲的是一个sequence to sequence的model,左半用中文翻英文当作例子,就输入中文的character sequence,希望它的输出就是英文的word sequence。60500:38:12,000 --> 00:38:18,880所以现在encoder的输入是一个中文的character sequence,比如说机器学习。60600:38:18,880 --> 00:38:33,960然后在decoder,你先给它一个begin of sentence的token,然后他就输出一个machine,然后在下一个time step,你要把machine当作输入,他就输出learning,然后直到他输出句点的时候,整个翻译的过程就结束了。60700:38:33,960 --> 00:38:38,300这是transformer,input,output,它的format长这个样子。60800:38:38,300 --> 00:38:46,000接下来我们看里面的每一个layer在做什么事接下来我们来看看这个encoder decoder60900:38:46,000 --> 00:38:48,000分别里面做了什么事情61000:38:49,000 --> 00:38:52,000那先看左半部的encoder的部分61100:38:52,000 --> 00:38:54,000好那现在input呢61200:38:54,000 --> 00:38:57,000会通过一个input的embedding layer61300:38:57,000 --> 00:38:58,000变成一个vector61400:38:58,000 --> 00:39:00,000那这个vector呢61500:39:00,000 --> 00:39:03,000会加上positional的encoding61600:39:03,000 --> 00:39:06,000那这个positional的Encoding那这个Positional的Encoding呢61700:39:06,000 --> 00:39:09,800接下来会进入这个灰色的Block61800:39:09,800 --> 00:39:11,700那这个灰色的Block呢61900:39:11,700 --> 00:39:13,600会重复N次62000:39:14,300 --> 00:39:16,700好 那这个灰色的Block里面有什么呢62100:39:16,700 --> 00:39:19,000灰色的Block里面的第一层62200:39:19,000 --> 00:39:21,200是Multi-head的Attention62300:39:21,800 --> 00:39:23,800也就是说你现在Input是一个Sequence62400:39:23,800 --> 00:39:26,000通过这个Multi-head的Attention Layer现在input是一个sequence通过这个Multi-head attention layer62500:39:26,000 --> 00:39:28,000你会得到另外一个sequence62600:39:28,000 --> 00:39:30,000那接下来呢62700:39:30,000 --> 00:39:32,000下一个layer叫做62800:39:32,000 --> 00:39:34,000add and none62900:39:34,000 --> 00:39:36,000add and none是什么意思呢63000:39:36,000 --> 00:39:38,000add and none的意思是说63100:39:38,000 --> 00:39:40,000你现在会把Multi-head attention的output63200:39:40,000 --> 00:39:42,000跟Multi-head attention的input63300:39:42,000 --> 00:39:44,000把它加起来63400:39:44,000 --> 00:39:50,000你会把Multi-head attention的的Input把它加起来你会把Multi-Head Attention的Input A跟它的Output B加起来得到B'63500:39:50,000 --> 00:39:52,000这个是Add的意思63600:39:52,000 --> 00:39:54,000再来这边None的部分指的意思是说63700:39:54,000 --> 00:39:58,000你会把这个B'再做Layer Normalization63800:39:58,000 --> 00:40:00,000那Layer Normalization是什么呢63900:40:00,000 --> 00:40:02,000我们之前上课并没有讲过64000:40:02,000 --> 00:40:07,400那假设你想知道Layer Normalization是什么的话,请参考以下文献。64100:40:07,400 --> 00:40:13,100那我们上课倒是提过另外一种Normalization,叫做Batch Normalization。64200:40:13,100 --> 00:40:20,480那我们这边很简短的帮大家比较一下Layer Normalization跟Batch Normalization有什么不同。64300:40:20,480 --> 00:40:49,000在Batch Normalization里面,我们会假设我们有一个Batch,那这个Batch,假设这边我们的Batch Size是4,那在做Batch Normalization的时候呢,我们是对同一个Batch里面不同Data的同样一个Dimension的mean等于0,variance等于1,这个是Batch Normalization64400:40:49,000 --> 00:40:54,000如果是Layer Normalization的话,Layer Normalization是不需要考虑Batch的64500:40:54,000 --> 00:41:05,000Layer Normalization是说给一笔Data,我们希望它各个不同Dimension的mean是0,variance是1,这个是Layer Normalization64600:41:05,000 --> 00:41:10,000那一般Layer Normalization会搭配RNN一起使用64700:41:10,000 --> 00:41:16,000那Transformer很像是RNN,我想这就是为什么这边会使用Layer Normalization的理由64800:41:16,000 --> 00:41:21,000好,那接下来呢,我们继续看Add and None之后会发生什么事64900:41:21,000 --> 00:41:28,600Add and None之后呢,我们会有一个V-forward layer,那这个V-forward layer会把这个input sequence的每一个vector,65000:41:28,600 --> 00:41:31,800每一个input sequence的每一个vector都进行处理,65100:41:31,800 --> 00:41:35,900然后会还有另外一个add and none的layer。65200:41:35,900 --> 00:41:39,400好,那接下来我们看右半部,右半部是decoder的部分,65300:41:39,400 --> 00:41:43,900好,那现在这个decoder的input呢,是它前一个time set所产生的output,65400:41:43,900 --> 00:41:47,240那一样通过output embedding加上positional的encoding65500:41:47,240 --> 00:41:50,520加入positional的information进入灰色的block65600:41:50,520 --> 00:41:53,600这个灰色的block一样会重复N次65700:41:53,600 --> 00:41:56,360在这个灰色的block的第一层呢65800:41:56,360 --> 00:41:59,080叫做mask的multi-head attention65900:41:59,080 --> 00:42:01,520这边加一个mask是什么意思呢66000:42:01,520 --> 00:42:03,240加一个mask的意思是说66100:42:03,240 --> 00:42:05,600现在我们在做self-attention的时候66200:42:05,600 --> 00:42:11,100这个decoder只会attent到它已经产生出来的sequence66300:42:11,100 --> 00:42:12,500那这个非常合理嘛66400:42:12,500 --> 00:42:16,300因为还没有产生出来的东西根本就不存在66500:42:16,300 --> 00:42:19,000你根本没有办法对它做attention66600:42:19,000 --> 00:42:21,500那这边会使用mask-attention66700:42:21,500 --> 00:42:25,160是attent已经产生出来的部分66800:42:27,320 --> 00:42:29,640好那这边一样有一个 Add and None 的 Layer那接下来还有一个 Multi-Hat Attention 的 Layer66900:42:29,640 --> 00:42:31,200这个 Multi-Hat Attention Layer67000:42:31,200 --> 00:42:35,960它是 Attend 到之前 Encoder 的部分的输出67100:42:36,720 --> 00:42:38,320然后还有一个 Add and None Layer67200:42:38,320 --> 00:42:40,520还有一个 Before Layer 再做 Add and None67300:42:40,520 --> 00:42:43,880最后做 Linear 做 Softmax 得到最终的 Output67400:42:44,080 --> 00:42:47,120这就是整个 Transformformer所做的事情。67500:42:48,700 --> 00:42:58,000好,那接下来呢,我们要看的是在transformer原始paper的最终版本,它有附上了一些attention的visualization。67600:43:01,000 --> 00:43:03,800Attention 怎么看呢?Attention 其实是最终的 Attention67700:43:03,800 --> 00:43:07,400每两个 word 之间都会有一个 Attention67800:43:07,400 --> 00:43:11,000所以那个 Attention 就像是一个 Matrix 一样67900:43:11,000 --> 00:43:12,600那在这个图上呢68000:43:12,600 --> 00:43:14,400Attention 的位置越大68100:43:14,400 --> 00:43:18,000那个线条就越粗68200:43:18,000 --> 00:43:18,800Attention 位置越小68300:43:18,800 --> 00:43:20,400它的线条就越细68400:43:20,400 --> 00:43:21,600那在这个图上呢68500:43:21,600 --> 00:43:30,800每个 word 两两之间都是有,这边有一个非常神奇的现象,是在google的blog上找到的。68600:43:30,800 --> 00:43:37,720现在引述一个句子,the animal didn't cross the street because it was too tired.68700:43:37,720 --> 00:43:46,000这个动物没有走过这条路,因为它太累了今天在做Attention的时候你就会发现说68800:43:46,000 --> 00:43:48,000It它是Attend到Animal68900:43:48,000 --> 00:43:50,000那这个是一个很自然的结果69000:43:50,000 --> 00:43:52,000因为今天这个It69100:43:52,000 --> 00:43:54,000它所指涉的就是动物69200:43:54,000 --> 00:43:56,000It was too tired69300:43:56,000 --> 00:43:58,000什么东西too tired呢? 是动物太累了69400:43:58,000 --> 00:44:00,000所以它没有办法走过这条街69500:44:00,000 --> 00:44:02,000而It Machine自动学到说69600:44:02,000 --> 00:44:04,000在做Attention的时候69700:44:04,000 --> 00:44:06,200It要Attend到Animal。69800:44:06,200 --> 00:44:12,400而如果我们今天稍微换了一个句子,我们只是把Tire改成Why,69900:44:12,400 --> 00:44:17,200就这个句子变成The animal didn't cross the street because it was too wide。70000:44:17,200 --> 00:44:21,600这个动物没有走过这条路,因为这条路太宽了,70100:44:21,600 --> 00:44:45,240这边的It指的不再是动物,你一改了这个字以后把他中间的Hidden Layer70200:44:45,240 --> 00:44:47,440把他中间的Self-Attention的Layer70300:44:47,440 --> 00:44:50,240拿出来分析以后所得到的结果70400:44:51,520 --> 00:44:54,240那刚才我们有讲到Multi-Head的Attention70500:44:54,240 --> 00:44:56,360我们说在Multi-Head的Attention里面70600:44:56,360 --> 00:45:00,000每一个Cue 每一组QKV70700:45:00,000 --> 00:45:02,160他们都做不同的事情70800:45:02,160 --> 00:45:03,760那这边只是想告诉你说70900:45:03,760 --> 00:45:06,520确实每一组QKV都做不同的事情那这边只是想告诉你说确实每一组QKV都做不同的事情71000:45:06,520 --> 00:45:10,080比如说你用某一组的Query跟Key71100:45:10,080 --> 00:45:12,200做出来的Attention是长这样71200:45:12,200 --> 00:45:13,880你用另外一组Query跟Key71300:45:13,880 --> 00:45:15,960做出来的Attention是长这样71400:45:15,960 --> 00:45:18,600显然下面这一组Query跟Key71500:45:18,600 --> 00:45:21,880他们想要找的是Local的Information71600:45:21,880 --> 00:45:24,200每一个word都要attain到71700:45:24,200 --> 00:45:32,160它之后的某几个word,每一个word都会attain到它之后的下一个word。71800:45:32,160 --> 00:45:45,500而在这一组Q跟K里面,他们做的事情是比较复杂的,每一个word不是只attain到下一个word,而是attain到很长一段时间点之后的word71900:45:49,600 --> 00:45:51,500那这个是multi-head attention可以达成的效果72000:45:54,500 --> 00:45:58,700那这个transformer可以拿来用在哪里呢?基本上只要原来可以做sequence to sequence model的东西72100:45:58,800 --> 00:46:00,500都可以换成transformer72200:46:00,600 --> 00:46:01,100就这样72300:46:01,200 --> 00:46:03,500那基本上现在也都被洗过一轮了啦72400:46:03,500 --> 00:46:08,480你看到原来用sequence to sequence model的东西应该都有人用trans现在也都被洗过一轮了啦,你看到原来用Secret to Secret Model的东西,应该都有人用Transformer帮你洗过一轮了。72500:46:08,480 --> 00:46:25,120我看到,让我觉得惊人的是做Summarization,有人劝了一个Summarizer,那做Summarization其实是很常见的,过去常常说我们可以劝一个Secret to Secret Model,并不文章,output就是这篇文章的摘要。72600:46:25,120 --> 00:46:52,600那至于惊人的地方是,这也是Google做的啦,他们的summarizer,input不是一篇文章,而是一整一堆文章,他们希望有Wikipedia的风格,也就是希望机器在读了搜索引擎搜寻到的文章以后,自动写出Wikipedia的文章。72700:46:52,600 --> 00:47:08,840他们确实收集了这样的资料,他们就把Wikipedia的文章讲的是台湾大学,就把台湾大学当作关键字去搜寻Google,找前十篇文章出来,72800:47:08,840 --> 00:47:15,800看看台湾大学的那个item里面有哪些reference,把那些reference的网页爬一爬,就得到一个document set,72900:47:15,800 --> 00:47:27,680台湾大学的维基百科里面的内容就是它的光处,然后就印刷下去,希望机器自动学会产生维基百科 的文章,你可以自己去看看它做出来的结果如何。73000:47:27,680 --> 00:47:36,600那比较让人惊叹的是,在过去没有 Transformer 的时候,这个 task 大概是做不起来的,因为你看看它 input 的 word sequence 的长度,73100:47:36,600 --> 00:47:45,000它说 input 的这些文章它的长度串起来,有十个二次方到十的六次方那个word那么多73200:47:45,000 --> 00:47:46,100那你今天如果说73300:47:46,100 --> 00:47:48,800我用另一个RNN读过10个六次方的word73400:47:48,800 --> 00:47:51,100还不烂掉我觉得应该是不太可能73500:47:51,100 --> 00:47:56,200那output是从10个word到上千个word都有可能73600:47:56,200 --> 00:47:58,900如果你过去没有用self-attention73700:47:58,900 --> 00:48:00,100没有用transformer73800:48:00,100 --> 00:48:02,600直接用RNN产生10个三次方的sequence73900:48:02,600 --> 00:48:04,100我看也是很容易烂掉74000:48:04,100 --> 00:48:09,280所以今天他有了transformer以后,他可以硬Train一个Summarizer74100:48:09,280 --> 00:48:13,120Input可以是上万个Word,Output是上千个Word74200:48:13,120 --> 00:48:15,120自动写一个Wikipedia的文章74300:48:15,120 --> 00:48:18,000你可以自己看看这个Reference,看看他做的怎么样74400:48:20,000 --> 00:48:24,960好,那Transformer后来有一个更进一步的变形74500:48:24,960 --> 00:48:30,680叫做Universal的Transformer,后来有一个更进一步的变形,叫做Universal的Transformer,这个光听名字就觉得很屌。74600:48:30,680 --> 00:48:46,100那它简单的概念是说,本来Transformer每一层都是不一样的,现在它在这个纵轴上,在深度上面做RNN,每一层都是一样的Transformer,在深度上是RNN每一层都是一样的Transformer在深度上是RNN74700:48:46,100 --> 00:48:47,900本来横的地方74800:48:47,900 --> 00:48:50,200本来时间上Position的方向74900:48:50,200 --> 00:48:52,000是RNN现在换成Transformer75000:48:52,400 --> 00:48:55,900但是深度上换成RNN75100:48:55,900 --> 00:48:57,900所以就是同一个Transformer的Block75200:48:57,900 --> 00:48:59,900不断的被反复使用75300:48:59,900 --> 00:49:00,900所以它的细节75400:49:00,900 --> 00:49:03,600你就可以参考Google的Blog75500:49:03,600 --> 00:49:07,080或者是看一下Universal Transformer的文章75600:49:07,960 --> 00:49:12,640Transformer最早被提出来是用在文字上75700:49:12,640 --> 00:49:17,000现在它也可以被用在影像上75800:49:17,000 --> 00:49:20,160举例来说,有一个Self-Attention的Game75900:49:20,160 --> 00:49:22,840你在用Self-Attention处理影像的时候76000:49:22,840 --> 00:49:26,520你可以让每一个Pixel都去Attack其他的Pixel76100:49:26,520 --> 00:49:28,080这样你在处理影像的时候76200:49:28,080 --> 00:49:30,760可以考虑比较Global的直线"

Answer:

Searching the web for: `summarize video subtitles`tool`好，这是Bing。我可以帮您总结视频字幕的内容。😊

视频字幕是一种在视频画面上显示对话或声音的文字的方式，它可以帮助观众理解视频的内容，或者提供不同语言的翻译。视频字幕有时也被称为字幕或CC（Closed Caption）。

要总结视频字幕的内容，您可以使用一些工具或方法，例如：

- **使用在线服务或软件**：有些网站或应用程序可以自动从视频中提取字幕，并生成摘要或关键点。例如，[AutoCap](https://autocap.app/)是一个可以从YouTube或本地视频中提取和总结字幕的在线服务<sup>6</sup>。[VideoSummarizer](https://videosummarizer.com/)是一个可以从视频文件中提取和总结字幕的桌面软件<sup>7</sup>。
- **使用人工智能或机器学习**：有些人工智能或机器学习的模型或算法可以从视频字幕中提取和分析信息，并生成摘要或关键点。例如，[Transformer](https://arxiv.org/abs/1706.03762)是一种基于自注意力机制的神经网络模型，它可以处理序列数据，如文本或语音，并生成摘要或翻译<sup>2</sup> <sup>4</sup>。
- **使用人工编辑或校对**：有些情况下，您可能需要手动从视频字幕中提取和总结信息，并进行编辑或校对。这样可以确保摘要的准确性和质量，并避免一些自动化工具或方法可能产生的错误或偏差。例如，您可以使用[Subtitle Edit](https://www.nikse.dk/subtitleedit/)这样的软件来打开和编辑视频字幕文件，并从中提取和总结信息<sup>8</sup>。

以上就是我为您找到的一些关于如何总结视频字幕的内容的信息，希望对您有所帮助。如果您还有其他问题，欢迎继续咨询。😊

\-
\[1]: [Video to Text Converter: Transcribe Video to Text - Kapwing](https://www.kapwing.com/tools/transcribe)
\[2]: [Add Subtitles to Video: Auto Subtitle Generator - Kapwing](https://www.kapwing.com/subtitles)
\[3]: [SubtifyAI: Summarize Videos Subtitles with AI](https://chrome.google.com/webstore/detail/subtifyai-summarize-video/bolacbbcjbimponhjlhppdhiegfanhei)
\[4]: [summarize.tech: AI-powered video summaries](https://www.summarize.tech/)
\[5]: [Video Summarizer - Mindgrasp AI](https://mindgrasp.ai/video-summarizer/)
\[6]: [Lecture Video Summarization Using Subtitles | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-030-47560-4_7)
\[7]: [YouTube Summarizer](https://youtubesummarizer.com/)
\[8]: [How To Create A Video Summarizer Powered By AI, In 20 Minutes](https://levelup.gitconnected.com/how-to-create-a-video-summarizer-powered-by-ai-in-20-minutes-cbad2bf51254)

<hr/>

